{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀 구현 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss=1.2676, 기울기 a=0.1849, 절편=-0.4334\n",
      "Epoch: 1000, loss=0.0557, 기울기 a=-1.6009, 절편=11.0208\n",
      "Epoch: 2000, loss=0.0361, 기울기 a=-2.0366, 절편=14.0909\n",
      "Epoch: 3000, loss=0.0269, 기울기 a=-2.3380, 절편=16.2087\n",
      "Epoch: 4000, loss=0.0214, 기울기 a=-2.5708, 절편=17.8417\n",
      "Epoch: 5000, loss=0.0178, 기울기 a=-2.7607, 절편=19.1734\n",
      "Epoch: 6000, loss=0.0152, 기울기 a=-2.9211, 절편=20.2982\n"
     ]
    }
   ],
   "source": [
    "data = [[2,0], [4,0], [6,0], [8,1], [10,1], [12,1], [14,1]]\n",
    "\n",
    "x_data = [i[0] for i in data]\n",
    "y_data = [i[1] for i in data]\n",
    "\n",
    "# 임의의 a,b 값을 변수로 정의\n",
    "a = tf.Variable(tf.random_normal([1], dtype=tf.float64, seed = 0))\n",
    "b = tf.Variable(tf.random_normal([1], dtype=tf.float64, seed = 0))\n",
    "\n",
    "# 시그모이드 함수 방정식 정의\n",
    "y = 1/(1+np.e**(a*x_data+b))\n",
    "\n",
    "# 오차 loss 구하는 함수\n",
    "loss = - tf.reduce_mean(np.array(y_data)*tf.log(y) + (1-np.array(y_data))*tf.log(1-y))\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.5\n",
    "\n",
    "# 오차 loss 값이 최소인 값 찾기\n",
    "gradient_descent = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# 텐서플로로 학습\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())  # 변수들을 멤리에 생성, 초기화\n",
    "    for step in range(6001):\n",
    "        sess.run(gradient_descent)\n",
    "        if step % 1000 == 0:\n",
    "            print(\"Epoch: %.f, loss=%.4f, 기울기 a=%.4f, 절편=%.4f\" %(step, sess.run(loss), sess.run(a), sess.run(b)))\n",
    "            \n",
    "# Epoch는 입력값에 대해 몇 번 반복되어 있는지 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[2,3],[4,3],[6,4],[8,6],[10,7],[12,8], [14,9]])\n",
    "y_data = np.array([0,0,0,1,1,1,1]).reshape(7,1)\n",
    "\n",
    "# 입력값을 placeholder에 저장\n",
    "X = tf.placeholder(tf.float64, shape=[None,2])\n",
    "Y = tf.placeholder(tf.float64, shape=[None,1])\n",
    "\n",
    "# 실행할 때마다 동일한 출력(결과)를 얻기 위한 값 설정\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# 임의의 a,b값을 변수로 정의\n",
    "a = tf.Variable(tf.random_uniform([2,1], dtype = tf.float64)) # [2,1]은 들어오는 값은 2개, 나가는 값은 1개\n",
    "b = tf.Variable(tf.random_uniform([1], dtype = tf.float64))\n",
    "\n",
    "# 시그모이드 함수 방정식 정의\n",
    "y = tf.sigmoid(tf.matmul(X,a)+b)\n",
    "\n",
    "# 오차 loss 구하는 함수\n",
    "loss = - tf.reduce_mean(Y*tf.log(y) + (1-Y*tf.log(1-y)))\n",
    "\n",
    "# 오차 loss 값이 최소인 값 찾는 식 정의\n",
    "gradient_descent = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.cast() 함수는 형변환(캐스팅) 수행\n",
    "- 부동소수점(실수)를 정수형으로 변환할 때는 소수점 이하 버림\n",
    "- bool 자료형을 정수형으로 변환하 때는 True는 1, False는 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss=-6.7614, 기울기1 a1=3.4338, 기울기2 a2=3.0384, 절편=0.4507\n",
      "Epoch: 500, loss=nan, 기울기1 a1=nan, 기울기2 a2=nan, 절편=nan\n",
      "Epoch: 1000, loss=nan, 기울기1 a1=nan, 기울기2 a2=nan, 절편=nan\n",
      "Epoch: 1500, loss=nan, 기울기1 a1=nan, 기울기2 a2=nan, 절편=nan\n",
      "Epoch: 2000, loss=nan, 기울기1 a1=nan, 기울기2 a2=nan, 절편=nan\n",
      "Epoch: 2500, loss=nan, 기울기1 a1=nan, 기울기2 a2=nan, 절편=nan\n",
      "Epoch: 3000, loss=nan, 기울기1 a1=nan, 기울기2 a2=nan, 절편=nan\n",
      "Epoch: 3500, loss=nan, 기울기1 a1=nan, 기울기2 a2=nan, 절편=nan\n",
      "Epoch: 4000, loss=nan, 기울기1 a1=nan, 기울기2 a2=nan, 절편=nan\n",
      "Epoch: 4500, loss=nan, 기울기1 a1=nan, 기울기2 a2=nan, 절편=nan\n",
      "Epoch: 5000, loss=nan, 기울기1 a1=nan, 기울기2 a2=nan, 절편=nan\n"
     ]
    }
   ],
   "source": [
    "predicted = tf.cast(y>0.5, dtype=tf.float64)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float64))\n",
    "\n",
    "# 텐서플로로 학습\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())  # 변수들을 멤리에 생성, 초기화\n",
    "    for step in range(5001):\n",
    "        a_,b_,loss_, _ = sess.run([a,b,loss,gradient_descent], feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 500 == 0:\n",
    "            print(\"Epoch: %.f, loss=%.4f, 기울기1 a1=%.4f, 기울기2 a2=%.4f, 절편=%.4f\" %(step, loss_, a_[0], a_[1], b_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
